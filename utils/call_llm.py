import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os

# –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏
if torch.cuda.is_available():
    print("üöÄ Using GPU - clearing memory...")
    torch.cuda.empty_cache()
    print(f"[–ü–∞–º—è—Ç—å –î–û –∑–∞–≥—Ä—É–∑–∫–∏] Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
MODEL_NAME = "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"
dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º truncation
print("\n–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    truncation=True,  # –Ø–≤–Ω–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ —É—Å–µ—á–µ–Ω–∏—è
    model_max_length=512  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
)

print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
    revision="gptq-4bit-32g-actorder_True"
)
print(f"[–ü–∞–º—è—Ç—å –ü–û–°–õ–ï –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏] Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
generator = pipeline(
    "text-generation", 
    model=model, 
    tokenizer=tokenizer,
    truncation=True,  # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
)
print(f"[–ü–∞–º—è—Ç—å –ü–û–°–õ–ï —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞] Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

def call_llm(prompt: str, max_tokens: int = 512) -> str:
    print(f"\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞... (–¥–æ: {torch.cuda.memory_allocated()/1024**3:.2f} GB)")
    result = generator(
        prompt, 
        max_length=max_tokens,
        truncation=True,  # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        do_sample=True, 
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )[0]["generated_text"]
    print(f"[–ü–∞–º—è—Ç—å –ü–û–°–õ–ï –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏] Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
    return result.replace(prompt, "").strip()